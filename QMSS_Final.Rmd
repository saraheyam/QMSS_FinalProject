---
title: "A Predictive Analysis of Doctor Performance Evaluation Data under the Merit-based Incentive Payment System (MIPS)"
author:
- "Xintao Ding (xd2222)" 
- "Sarah Yam (sy2806)"
output:
  pdf_document: default
  html_notebook: default
---

# explain mips basically?

The Physician Compare website was created by the Centers for Medicare & Medicaid Services (CMS) in December 2010 as required by the Affordable Care Act (ACA) of 2010 to help patients assess and find doctors and hospitals. This dataset contains the information supplied to patients via that website, including patient satisfaction surveys and performance scores across over 100 metrics.

Looking at individual physician scores:  

* MIPS
* Performance by measure category
* Organization MIPS

__Question/Problem:__ How can we better help patients assess and find doctors, where the scoring and rating come in a format not easily accessible or understandable by the average individual?  
  
__Approach/Methods:__ Supervised learning for binary classification utilizing the MIPS as a target with other physician scoring methods as predictors (which we know some of the metrics are direct factors of the individual MIPS scoring, such as the IA, ACI, and Quality category scorings). Potential methods outlined below, including generalized linear models and tree methods.

Potential Methods for Binary Classification:  
Using overall MIPS for individuals where MIPS>= 75, the positive payment adjustment threshold.

* could apply spline to other MIPS, ACI scorings since they're somewhat discrete in nature.  
    + ACI >= 0 : clinician reported ACI category
    + ACI >= 50: clinician achieved base score for ACI
    + MIPS < 30: Negative Payment Adjustment
* Predictive MIPS >= 75, essentially.
* Methods to try:  
    + PCA to explore relationship of numeric variables 
    + PCA to explor clustering of observations
    + glmnet for binary classification (elastic model/penalized logit)
    + glm logit model with polynomials?
    + tree model if we can make it work? (Single Tree, Random Forest, Boosting, Dbarts???)
    + PLSDA or LDA
    + nnet or MARS
    
# explain why some of these methods?
# explain data -- cite where it came from 

```{r, message = F, collapse = T, warning = F}
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(stringr))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(dmm))
suppressPackageStartupMessages(library(pcaPP))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(splines))
suppressPackageStartupMessages(library(dbarts))
```

```{r}
set.seed(70856775)
```

```{r}
# Professional.Enrollment.ID is non-predictive, same with PAC_id
dr_scores <- read.csv("full_doctor_scoring.csv", 
                      sep = ",", na = c("NA", "N/A"))
# remove majority of missingness by removing each task type
no_tasks <- dr_scores[c(1:23, 57)] 
# add a "UKN" for unknown category
no_tasks$Credential <- factor(no_tasks$Credential, 
                              levels = c(levels(dr_scores$Credential), 
                                         "UKN"))
no_tasks$Credential[is.na(no_tasks$Credential)] <- "UKN"
# -1 for not reporting individual ACI Score (NA value)
no_tasks$ACI_Score_Dr[is.na(no_tasks$ACI_Score_Dr)] <- -1
```

### PCA to Explore Correlation of Variables

An initial PCA analysis to look understand how rows cluster based on the column variables. This is without considering the binary classifying aspect of MIPS >= 75,, and is just to understand some of the structure of the scores in the data. 

```{r, cache = T}
# still minor missingness in Graduation Year and Quality Score
numerics <- no_tasks[, c(8, 10:11, 15:18, 20:24)] %>% na.omit(.)
pr_out <- PCAproj(numerics, scale = sd)
par(mar = c(5, 4, 3, 3) + 0.1, las = 1)

# visualization of any immediate outliers and the variables
biplot(pr_out, scale = 0, cex = c(0.6, 0.8))
# clusters within the observations visible
biplot(pr_out, scale = 0, cex = c(0.13, 0.8)) 
```

As mostly expected, the MIPS scores for the hospital organizations that each doctor works at are more correlated to each other than they are to the MIPS scores for each individual doctor. While the number of secondary specialties each doctor has is more correlated to the individual doctor MIPS scores, the group practice ID is more correlated to the organization scores. This is also fairly in line with our expectations that measures for the practice organization would cluster separately from the measures for the individual doctors. Interestingly, the average performance for an individual doctor across task categories seems to be more correlated to the organization scores though. Since performance is measured though individual patient reporting, their experience with the organization itself may be taken into consideration and bias their score, even if the doctor-patient interaction itself was positive.  
  
In the second PCA plot, with the observation labels less cluttered from size, we see that there seem to be distinct clusters of observations.  This would indicate groupings within the observations with distinguishing measurement characteristics. Many points do deviate from the groupings themselves. Still, overall, it doesn't appear that any observations seems like a major outlier, as seen in the first plot.


#### Prepare dataset for training/testing


```{r}
# we do not expect PAC_id or Proefssional.Enrollment.ID to be predictive
# create factor for classification prediction (Individual Doctor MIPS Score >= 75)
# Score_source_org has no variance -- they're all group

no_tasks <- na.omit(no_tasks)[, c(3:13, 15:ncol(no_tasks))]
no_tasks$MIPS75_Dr <- factor(no_tasks$MIPS_Score_Dr >= 75.0, 
                             levels = c(TRUE, FALSE), 
                             labels = c("yes", "no"))

# too many levels for partitioning separately
no_tasks$Prim.Schl <- interaction(no_tasks$Primary.specialty, 
                                  no_tasks$Medical.school.name, 
                                  sep = ":", drop = TRUE)



# variables we expect to be predictive
preds <- c("Gender", "Prim.Schl", "MIPS_Score_Org",
           "Num.Secondaries", "Quality_Score_Org", 
           "IA_Score_Org", "IA_Score_Dr", "Graduation.year", 
           "ACI_Score_Dr", "Quality_Score_Dr", 
           "avg.perf", "MIPS75_Dr", "MIPS_Score_Dr")
no_tasks <- no_tasks[, preds]


remain <- group_by(no_tasks, Prim.Schl) %>% 
  summarise(., count = n()) %>%
  filter(., count > 50) %>%
  .$Prim.Schl 
no_tasks <- subset(no_tasks, Prim.Schl %in% remain)

numerical <-  no_tasks[, c("MIPS_Score_Org", "Num.Secondaries", 
                          "Quality_Score_Org", "IA_Score_Org", 
                          "IA_Score_Dr", "Graduation.year", 
                          "ACI_Score_Dr", "Quality_Score_Dr", 
                          "avg.perf",  "MIPS_Score_Dr")]

no_tasks <- no_tasks[, c("Gender", "Prim.Schl", "MIPS_Score_Org",
           "Num.Secondaries", "Quality_Score_Org", "IA_Score_Org", 
           "IA_Score_Dr", "Graduation.year", "ACI_Score_Dr", 
           "Quality_Score_Dr", "avg.perf", "MIPS75_Dr")]
```



## Maybe explore PCA again for the things we are directly looking at and using the data that we've trimmed down to

```{r, cache = T}

pca_out <- PCAproj(numerical, scale = sd)

par(mar = c(5, 4, 3, 3) + 0.1, las = 1)

# visualization of any immediate outliers and the variables
biplot(pca_out, scale = 0, cex = c(0.6, 0.8))
# clusters within the observations visible
biplot(pca_out, scale = 0, cex = c(0.13, 0.8)) 
```

When comparing just the numeric components of the preditors we will be training and testing on, scores measuring performance of individual doctors cluster apart from scores measuring performance of health care organizations. This makes sense as the criteria for evaluating an organization are likely more related to each other than they would be to the criteria for evaluating individual health care providers. Included in the doctor score cluster is the MIPS Score for individual dcotors, which is target variable we are trying to predict. The MIPS Score for individual doctors though, still seems to be fairly correlated with the organization-based scores. Overall though, we don't really appear to see any major outliers from the observations.

Similar to the original PCA plot as well, the average performance seems more correlated to the organization scores than the actual invidividual doctor scores, despite it being a measurement of average performance for individual doctors. Performance though, was measured across a large variety of different tasks, including e-Prescribing, Preventative Care and Screening, Diabetic Care, Nuclear Medicine, Patient Portal Access. Given the vast range of topics covered, it's apparent that many of these tasks, such as online patient portal access or disease screening, are more dependent on services provided and resources of the organization, rather than the individual caregiver.  

The plot with smaller points again also shows us that the observations do a appear to group in distinctive patterns still, hopefully indicating that some of the underlying structure and characteristics of the data were still retained after further processing of the data.


```{r, message = F, warning = F}
in_train <- createDataPartition(no_tasks$Prim.Schl,
                               p = 0.8, list = F)

training <- no_tasks[in_train, ]
testing <- no_tasks[-in_train, ]
```


### Let's test with a basic linear model:

```{r}
# testing without stratification, med school name, primary speciality
ols <- lm(MIPS75_Dr == "yes" ~ ., data = training)
yhat <- predict(ols, newdata = testing)
z_ols <- factor(yhat > 0.5, levels = c(TRUE, FALSE), 
                labels = c("yes", "no"))
confusionMatrix(z_ols, reference = testing$MIPS75_Dr)

calibration(MIPS75_Dr ~ yhat, data = testing) %>%
  plot(.)
```

Where, an ideally calibrated model should see 20% of observations being successful that have a predicted probability of about 0.2 if it finds that there's a 0.2 probability of success, our model seems more extreme, in almost a binary manner. In this case, when our models finds less than 0.5 probability of success, the proportion seen of observations being successful is far lower than that probability of success. In fact, none of the observation with a predicted probability greater than 0 and less than ~0.4 are successful. On the other hand, we see that that the proportion of observations being successful is far larger than the probability of success when the model says there's greater than ~0.6 probability of success. At a 0.8 probability of success, we actually see nearly 100% of those observations being successful. While the accuracy from this model is still quite good (not entirely unexpected since our dataset is fairly large and comprehensive), our data does not appear to quite fit an exact linear model, and would likely perform better with more flexible models. 

#### GLM Models -- Logit and GLMnet Penalized

```{r}
# continous measurements can be scaled
# ACI is really a mixture of discrete and continuous 

pp_names <- c("Num.Secondaries", "Quality_Score_Org",  
              "Quality_Score_Dr", "IA_Score_Org", 
              "IA_Score_Dr", "avg.perf")
pp <- list(center = pp_names, scale = pp_names)
```


```{r}
in_train <- createDataPartition(no_tasks$MIPS75_Dr,
                                p = 0.6, list = F)
training <- no_tasks[in_train, ]
testing <- no_tasks[-in_train, ]
```

```{r, message = F, warning = F}
logit <- glm(MIPS75_Dr ~ ., data = training, 
             family = binomial(link = "logit"))
z <- predict(logit, newdata = testing, 
             type = "response") > 0.5
z <- factor(z, levels = c(TRUE, FALSE), 
            labels = c("no", "yes"), order = T)
confusionMatrix(z, testing$MIPS75_Dr)
```


```{r, message = F, warning = F, cache = T}
# glmnet PENALIZATION
ctrl <- trainControl(method = "cv", number = 3)
enet <- train(formula(logit), data = training, 
              method = "glmnet", trControl = ctrl, 
              tuneLength = 10, preProcess = pp)
enet_hat <- predict(enet, newdata = testing)
confusionMatrix(enet_hat, reference = testing$MIPS75_Dr)
```

# Analysis of GLM models, with and without penalization. Between each other and compared to linear.


#### Linear Models with Polynomials and Splines -- how linear is our data? Do splines make it better? 

# explain why the splines and polynomials  

Potential polynomials:  

- Num.Secondaries

Potential splines:  

- ACI: spline at -1:0, 0:50, 50:
- raw MIPS: spline at 0:30, 30:75, 75:

```{r, message = F, warning = F}
# with polynomials and splines
poly <- glm(MIPS75_Dr ~ . + 
              bs(MIPS_Score_Org, knots = c(30, 75)) + 
              bs(ACI_Score_Dr, knots = c(0, 50)) + 
              poly(Num.Secondaries, degree = 2), 
            data = training, 
            family = binomial(link = "logit"))
poly_z <- predict(poly, newdata = testing, 
                  type = "response") > 0.5
poly_z <- factor(poly_z, levels = c(TRUE, FALSE), 
                 labels = c("no", "yes"), order = T)
confusionMatrix(poly_z, testing$MIPS75_Dr)
```

```{r, message = F, warning = F, cache = T}
# glmnet PENALIZATION
# with polynomials and splines
poly_el <- train(formula(poly), data = training, 
                 method = "glmnet", trControl = ctrl, 
                 tuneLength = 10, preProcess = pp)
poly_yh <- predict(poly_el, newdata = testing)
confusionMatrix(poly_yh, reference = testing$MIPS75_Dr)
```


#### Tree Methods

```{r}
# training = 3587 yes, 1793 no (yes = .66)
# testing = 2391 yes, 1194 no (yes = 0.66)
# at random, it would be gussing 50% right, at only predicting yes or no, it would be guessing 33 or 66% right
# 80-90% are great trajectories then

```

#### Basic Single Tree Model on the same predictors
```{r}
one_tree <- train(MIPS75_Dr ~ ., data = training, method = "rpart2", 
                  tuneLength = 10, trControl = ctrl, preProcess = pp)
plot(one_tree)
confusionMatrix(testing$MIPS75_Dr, predict(one_tree, newdata = testing))
```

# analysis of graph?

#### DBarts: Bayesian Additive Regression Trees
```{r}
bart_out <- bart2(MIPS75_Dr == "yes" ~ ., data = training, 
                  test = testing, k = 5, base = 0.4, 
                  power = 1.3, keepTrees =  T)
confusionMatrix(factor(
  apply(pnorm(bart_out$yhat.test), 3, mean) > 0.5,
  levels = c(T, F), labels = levels(testing$MIPS75_Dr)),
  reference = testing$MIPS75_Dr)
```

# analysis comparison of trees, difference in trees (no need to interactions, etc because learned)



#### NNET
```{r, cache = T}
nnetGrid <- expand.grid(.decay = c(0, 0.01, .01),
                        .size = c(1:3))
nn <- train(MIPS75_Dr ~ . -Prim.Schl, data = training, method = "nnet", 
            tuneLength = 10, trControl = ctrl, tuneGrid = nnetGrid,
            preProcess = pp, trace = FALSE)

## prim.schl was taken out as it is a zero-variance predictor

confusionMatrix(testing$MIPS75_Dr, predict(nn, newdata = testing))
```

#### MARS
```{r message = F, warning = F,cache = T }
marsGrid <- expand.grid(.degree = 1:3, .nprune = 1:10)
MARS <- train(MIPS75_Dr ~ . -Prim.Schl, data = training, method = "earth", 
                  tuneLength = 10, trControl = ctrl, preProcess = pp, trace = FALSE)
## prim.schl was taken out as it is a zero-variance predictor

confusionMatrix(testing$MIPS75_Dr, predict(MARS, newdata = testing))


```

"perfecrt separation happened, doesn't matter in an earth context since earth doesn't use t-value and other statistics that will be unrealiable for subsequent inference. Mostly likely happens in a cross-validation model since we are looking at smaller datasets"  


# final nnet/MARS analysis

# overview of models and what worked best. why? 
# what did this tell us and how did this help solve problem?







