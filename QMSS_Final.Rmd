---
title: "QMSS Final Project"
output:
  pdf_document: default
  html_notebook: default
---

The Physician Compare website was created by the Centers for Medicare & Medicaid Services (CMS) in December 2010 as required by the Affordable Care Act (ACA) of 2010 to help patients assess and find doctors and hospitals. This dataset contains the information supplied to patients via that website, including patient satisfaction surveys and performance scores across over 100 metrics.

Looking at individual physician scores:  

* MIPS
* Performance by measure category
* Organization MIPS

__Question/Problem:__ How can we better help patients assess and find doctors, where the scoring and rating come in a format not easily accessible or understandable by the average individual?  
  
__Approach/Methods:__ Supervised learning for binary classification utilizing the MIPS as a target with other physician scoring methods as predictors (which we know some of the metrics are direct factors of the individual MIPS scoring, such as the IA, ACI, and Quality category scorings). Potential methods outlined below, including generalized linear models and tree methods.

Potential Methods for Binary Classification:  
Using overall MIPS for individuals where MIPS>= 75, the positive payment adjustment threshold.

* could apply spline to other MIPS, ACI scorings since they're somewhat discrete in nature.  
    + ACI >= 0 : clinician reported ACI category
    + ACI >= 50: clinician achieved base score for ACI
    + MIPS < 30: Negative Payment Adjustment
* Predictive MIPS >= 75, essentially.
* Methods to try:  
    + glmnet for binary classification (elastic model/penalized logit)
    + glm logit model with polynomials?
    + tree model if we can make it work? (Single Tree, Random Forest, Boosting, Dbarts???)
    + PLSDA or LDA
    + nnet or MARS

```{r, message = F, collapse = T, warning = F}
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(stringr))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(dmm))
suppressPackageStartupMessages(library(pcaPP))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(splines))
```

```{r}
set.seed(70856775)
```

```{r}
# Professional.Enrollment.ID is non-predictive, same with PAC_id
dr_scores <- read.csv("full_doctor_scoring.csv", 
                      sep = ",", na = c("NA", "N/A"))
# remove majority of missingness by removing each task type
no_tasks <- dr_scores[c(1:23, 57)] 
# add a "UKN" for unknown category
no_tasks$Credential <- factor(no_tasks$Credential, 
                              levels = c(levels(dr_scores$Credential), 
                                         "UKN"))
no_tasks$Credential[is.na(no_tasks$Credential)] <- "UKN"
# -1 for not reporting individual ACI Score (NA value)
no_tasks$ACI_Score_Dr[is.na(no_tasks$ACI_Score_Dr)] <- -1
```

### PCA to Explore Correlation of Variables

An initial PCA analysis to look understand how rows cluster based on the column variables. This is without considering the binary classifying aspect of MIPS >= 75,, and is just to understand some of the structure of the scores in the data. 

```{r, cache = T}
# still minor missingness in Graduation Year and Quality Score
numerics <- no_tasks[, c(8, 10:11, 15:18, 20:24)] %>% na.omit(.)
pr_out <- PCAproj(numerics, scale = sd)
par(mar = c(5, 4, 3, 3) + 0.1, las = 1)

# visualization of any immediate outliers and the variables
biplot(pr_out, scale = 0, cex = c(0.6, 0.8))
# clusters within the observations visible
biplot(pr_out, scale = 0, cex = c(0.13, 0.8)) 
```

As mostly expected, the MIPS scores for the hospital organizations that each doctor works at are more correlated to each other than they are to the MIPS scores for each individual doctor. While the number of secondary specialties each doctor has is more correlated to the individual doctor MIPS scores, the group practice ID is more correlated to the organization scores. This is also fairly in line with our expectations that measures for the practice organization would cluster separately from the measures for the individual doctors. Interestingly, the average performance for an individual doctor across task categories seems to be more correlated to the organization scores though. Since performance is measured though individual patient reporting, their experience with the organization itself may be taken into consideration and bias their score, even if the doctor-patient interaction itself was positive.  
  
In the second PCA plot, with the observation labels less cluttered from size, we see that there seem to be distinct clusters of observations.  This would indicate groupings within the observations with distinguishing measurement characteristics. Many points do deviate from the groupings themselves. Still, overall, it doesn't appear that any observations seems like a major outlier, as seen in the first plot.


#### Prepare dataset for training/testing


```{r}
# we do not expect PAC_id or Proefssional.Enrollment.ID to be predictive
# create factor for classification prediction (Individual Doctor MIPS Score >= 75)
# Score_source_org has no variance -- they're all group

no_tasks <- na.omit(no_tasks)[, c(3:13, 15:ncol(no_tasks))]
no_tasks$MIPS75_Dr <- factor(no_tasks$MIPS_Score_Dr >= 75.0, 
                             levels = c(TRUE, FALSE), 
                             labels = c("yes", "no"))

# too many levels for partitioning separately
no_tasks$Prim.Schl <- interaction(no_tasks$Primary.specialty, 
                                  no_tasks$Medical.school.name, 
                                  sep = ":", drop = TRUE)

# variables we expect to be predictive
preds <- c("Gender", "Prim.Schl", "MIPS_Score_Org",
           "Num.Secondaries", "Quality_Score_Org", 
           "IA_Score_Org", "IA_Score_Dr", "Graduation.year", 
           "ACI_Score_Dr", "Quality_Score_Dr", 
           "avg.perf", "MIPS75_Dr")
no_tasks <- no_tasks[, preds]


remain <- group_by(no_tasks, Prim.Schl) %>% 
  summarise(., count = n()) %>%
  filter(., count > 50) %>%
  .$Prim.Schl 

no_tasks <- subset(no_tasks, Prim.Schl %in% remain)
```

## Maybe explore PCA again for the things we are directly looking at and using the data that we've trimmed down to



```{r, message = F, warning = F}
in_train <- createDataPartition(no_tasks$Prim.Schl,
                               p = 0.8, list = F)

training <- no_tasks[in_train, ]
testing <- no_tasks[-in_train, ]
```


### Let's test with a basic linear model:

```{r}
# testing without stratification, med school name, primary speciality
ols <- lm(MIPS75_Dr == "yes" ~ ., data = training)
yhat <- predict(ols, newdata = testing)
z_ols <- factor(yhat > 0.5, levels = c(TRUE, FALSE), 
                labels = c("yes", "no"))
confusionMatrix(z_ols, reference = testing$MIPS75_Dr)

calibration(MIPS75_Dr ~ yhat, data = testing) %>%
  plot(.)
```

# ADJUST ANALYSIS FOR CHANGES IN FACTORIZATION

The stratified model seems to fit a bit better compared to the model trained and tested without medical school name or primary speciality. This is especially for bins of lower probabilities, such as 0.5, which is fairly expected, as a larger number of observations from stratificaion and including medical school name and primary specialty would likely provide much more information. These factors, medical school name and primary specialty, probably strongly influence scores. Both the doctor's medical school name and the types of diseases and treatments could bias how scorers evaluate. For example, a scorer may be predisposed to giving a higher score if the doctor being evaluated attended a more prestigious school. Certain fields of medicine, such as surgery or oncology, could incur much higher costs or have higher mortality rates compared to fields such as family medicine, skewing factors of the final MIPS score, such as cost and quality scoring. Still, even with fairly good accuracies, we can see that the model isn't calibrated in an ideal manner.

#### GLM Models -- Logit and GLMnet Penalized

```{r}
# continous measurements can be scaled
# ACI is really a mixture of discrete and continuous 

pp_names <- c("Num.Secondaries", "Quality_Score_Org",  
              "Quality_Score_Dr", "IA_Score_Org", 
              "IA_Score_Dr", "avg.perf")
pp <- list(center = pp_names, scale = pp_names)
```


```{r}
in_train <- createDataPartition(no_tasks$MIPS75_Dr,
                                p = 0.6, list = F)
training <- no_tasks[in_train, ]
testing <- no_tasks[-in_train, ]
```

```{r, message = F, warning = F}
logit <- glm(MIPS75_Dr ~ ., data = training, 
             family = binomial(link = "logit"))
z <- predict(logit, newdata = testing, 
             type = "response") > 0.5
z <- factor(z, levels = c(TRUE, FALSE), 
            labels = c("no", "yes"), order = T)
confusionMatrix(z, testing$MIPS75_Dr)
```


## Wow glm fit way better than I thought so quickly


```{r, message = F, warning = F, cache = T}
# glmnet PENALIZATION
ctrl <- trainControl(method = "cv", number = 3)
enet <- train(formula(logit), data = training, 
              method = "glmnet", trControl = ctrl, 
              tuneLength = 10, preProcess = pp)
enet_hat <- predict(enet, newdata = testing)
confusionMatrix(enet_hat, reference = testing$MIPS75_Dr)
```


#### Linear Models with Polynomials and Splines -- how linear is our data? Do splines make it better? WHY IS IT LEARNING SO WELL ALREADY

Potential polynomials:  

- Num.Secondaries

Potential splines:  

- ACI: spline at -1:0, 0:50, 50:
- raw MIPS: spline at 0:30, 30:75, 75:

```{r, message = F, warning = F}
# with polynomials and splines
poly <- glm(MIPS75_Dr ~ . + 
              bs(MIPS_Score_Org, knots = c(30, 75)) + 
              bs(ACI_Score_Dr, knots = c(0, 50)) + 
              poly(Num.Secondaries, degree = 2), 
            data = training, 
            family = binomial(link = "logit"))
poly_z <- predict(poly, newdata = testing, 
                  type = "response") > 0.5
poly_z <- factor(poly_z, levels = c(TRUE, FALSE), 
                 labels = c("no", "yes"), order = T)
confusionMatrix(poly_z, testing$MIPS75_Dr)
```

```{r, message = F, warning = F, cache = T}
# glmnet PENALIZATION
# with polynomials and splines
poly_el <- train(formula(poly), data = training, 
                 method = "glmnet", trControl = ctrl, 
                 tuneLength = 10, preProcess = pp)
poly_yh <- predict(poly_el, newdata = testing)
confusionMatrix(poly_yh, reference = testing$MIPS75_Dr)
```


## DBARTS/trees and then NNET?


###### SO THIS CELL TAKES THE DATA, MANIPULATES FACTORS FOR PREDICTION AND PARITIONING. IF YOU WANT TO CHANGE WHICH CATEGORIES YOU WANT TO PREDICT ON, IT SHOULD PROBABLY BE IN HERE, USING THE "PREDS" VARIABLE DEFINITION.
```{r}
# Professional.Enrollment.ID is non-predictive, same with PAC_id
dr_scores <- read.csv("full_doctor_scoring.csv", 
                      sep = ",", na = c("NA", "N/A"))
# remove majority of missingness by removing each task type
no_tasks <- dr_scores[c(1:23, 57)] 
# add a "UKN" for unknown category
no_tasks$Credential <- factor(no_tasks$Credential, 
                              levels = c(levels(dr_scores$Credential), 
                                         "UKN"))
no_tasks$Credential[is.na(no_tasks$Credential)] <- "UKN"
# -1 for not reporting individual ACI Score (NA value)
no_tasks$ACI_Score_Dr[is.na(no_tasks$ACI_Score_Dr)] <- -1

no_tasks <- na.omit(no_tasks)[, c(3:13, 15:ncol(no_tasks))]
no_tasks$MIPS75_Dr <- factor(no_tasks$MIPS_Score_Dr >= 75.0, 
                             levels = c(TRUE, FALSE), 
                             labels = c("yes", "no"))

# too many levels for partitioning separately
no_tasks$Prim.Schl <- interaction(no_tasks$Primary.specialty, 
                                  no_tasks$Medical.school.name, 
                                  sep = ":", drop = TRUE)

# variables we expect to be predictive
preds <- c("Gender", "Prim.Schl", "MIPS_Score_Org",
           "Num.Secondaries", "Quality_Score_Org", 
           "IA_Score_Org", "IA_Score_Dr", "Graduation.year", 
           "ACI_Score_Dr", "Quality_Score_Dr", 
           "avg.perf", "MIPS75_Dr")
no_tasks <- no_tasks[, preds]


remain <- group_by(no_tasks, Prim.Schl) %>% 
  summarise(., count = n()) %>%
  filter(., count > 50) %>%
  .$Prim.Schl 

no_tasks <- subset(no_tasks, Prim.Schl %in% remain)
```


##### THIS IS THE TRAINING PARTITIONING CELL WE'LL WANT TO USE FOR THE DBARTS AND NNET STUFF

```{r}
in_train <- createDataPartition(no_tasks$MIPS75_Dr,
                                p = 0.6, list = F)
training <- no_tasks[in_train, ]
testing <- no_tasks[-in_train, ]
```

